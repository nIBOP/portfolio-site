---
slug: elt-pipeline
title: ELT-пайплайн
summary: Airflow, инкременты, качество данных, оповещения, эксплуатация.
date: 2024-10-24
tags: ['data-engineering', 'automation', 'python']
---

# ELT-пайплайн

## Обзор проекта

Проект представляет собой полнофункциональный ELT-пайплайн для автоматизированной обработки входящей документации с последующей загрузкой в аналитическое хранилище. Система построена на базе Apache Airflow и обеспечивает надежную, масштабируемую обработку данных с контролем качества на каждом этапе. Не единственный пайплайн, потом другие опишу тоже.

## Архитектура решения

### Основные компоненты

1. **Apache Airflow** — оркестрация и управление рабочими процессами
2. **MinIO S3** — надежное хранилище для входящих данных и промежуточных результатов
3. **ClickHouse** — колоночная СУБД для аналитических запросов и хранения обработанных данных

### Этапы обработки данных

#### 1. Загрузка данных в S3
- Прием входящей документации
- Организация файловой структуры с партиционированием по датам
- Проверка дублей файлов, черновиков - приведение к единому формату
- Версионирование загруженных данных
- Логирование метаданных о загрузке (размер, источник, timestamp)

#### 2. Проверка качества данных
- **Бизнес-правила**: проверка допустимых диапазонов значений, форматов дат
- **Полнота данных**: отсутствие критичных пропусков

При обнаружении критических ошибок пайплайн останавливается с детальным отчетом о проблемах.

#### 3. Загрузка в ClickHouse
- Трансформация данных в оптимальный для ClickHouse формат
- Инкрементальная загрузка для минимизации нагрузки
- Использование движка MergeTree для эффективного хранения
- Создание материализованных представлений для часто используемых агрегаций
- Партиционирование таблиц по временным меткам

## Технические особенности

### Управление зависимостями
- DAG-структура с четкими зависимостями между задачами
- Retry-стратегии для обработки временных сбоев
- Timeout-контроль для предотвращения зависаний

### Мониторинг и оповещения
- Дашборды в Airflow для мониторинга выполнения
- Детальное логирование на всех этапах

### Производительность
- Параллельное выполнение независимых задач
- Оптимизация запросов к ClickHouse
- Batch-обработка для больших объемов данных
- Кэширование промежуточных результатов
- Backfills для обновления статусов

## Результаты внедрения

- **Автоматизация**: Полностью автоматизированный процесс без ручных интервенций
- **Надежность**: Контроль качества на каждом этапе, минимизация ошибок в конечных данных
- **Скорость**: Инкрементальная загрузка сокращает время обработки
- **Масштабируемость**: Архитектура позволяет легко добавлять новые источники данных
- **Прозрачность**: Полная видимость процесса обработки через Airflow UI

## Используемые технологии

- **Python** — основной язык разработки
- **Apache Airflow** — оркестрация пайплайнов
- **ClickHouse** — аналитическая СУБД
- **MinIO S3** — объектное хранилище
- **Kubernetes** — контейнеризация компонентов
- **Git** — версионирование кода DAG'ов

## Перспективы развития

- Внедрение Data Lineage для отслеживания происхождения данных
- Интеграция с dbt для более сложных трансформаций
- Добавление ML-моделей для прогнозирования аномалий в данных

## Визуальные материалы

![Скриншот запроса к кликхаусу](/img/tickets_db.png)

![Скриншот даг в Airflow](/img/airflow tickets dags.jpg)

![Скриншот Minio Bucket'а](/img/minio bucket.jpg)